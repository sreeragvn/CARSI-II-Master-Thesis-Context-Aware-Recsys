{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_context_data = False\n",
    "regenerate_context_data = True\n",
    "sequence_augmentation = True\n",
    "whole_session_context = False\n",
    "model_test_run = True\n",
    "\n",
    "PATH_TO_LOAD = '../data/04_Merged'\n",
    "combined_context_path = '../data/05_Interaction_Sequences/context.csv'\n",
    "\n",
    "window = 100 #seconds\n",
    "\n",
    "base_path = '../datasets/sequential/'\n",
    "augmentation_folder = 'aug/' if sequence_augmentation else 'non_aug/'\n",
    "if model_test_run:\n",
    "    augmentation_folder = 'test/aug/' if sequence_augmentation else 'test/non_aug/'\n",
    "\n",
    "sequence_context_path = f'{base_path}{augmentation_folder}parameters/sequence_context.csv'\n",
    "parameter_path = f'{base_path}{augmentation_folder}parameters/param.pkl'\n",
    "train_session_path = f'{base_path}{augmentation_folder}parameters/train_sessions.pkl'\n",
    "test_session_path = f'{base_path}{augmentation_folder}parameters/test_sessions.pkl'\n",
    "train_dynamic_context_path = f'{base_path}{augmentation_folder}dynamic_context/train.csv'\n",
    "test_dynamic_context_path = f'{base_path}{augmentation_folder}dynamic_context/test.csv'\n",
    "train_static_context_path = f'{base_path}{augmentation_folder}static_context/train.csv'\n",
    "test_static_context_path = f'{base_path}{augmentation_folder}static_context/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = ['index', 'avg_irradiation', 'steering_speed', 'temperature_out', 'hour',\n",
    "       'month', 'odometer', 'light_sensor_rear', 'light_sensor_front',\n",
    "       'temperature_in', 'KBI_speed', 'soc', 'ESP_speed', 'latitude',\n",
    "       'longitude', 'seatbelt_codriver', 'seatbelt_rear_l', 'seatbelt_rear_m',\n",
    "       'seatbelt_rear_r', 'CHA_ESP_drive_mode', 'CHA_MO_drive_mode',\n",
    "       'rain_sensor', 'street_category', 'kickdown', 'altitude',\n",
    "       'driving_program', 'datetime', 'session', 'Label', 'ID',\n",
    "       'FunctionValue', 'domain', 'BeginTime', 'time_second',\n",
    "       'distance_driven', 'ts_normalized', 'weekday']\n",
    "\n",
    "selected = [ 'avg_irradiation', 'steering_speed', 'temperature_out', 'hour',\n",
    "       'month', 'light_sensor_rear', 'light_sensor_front',\n",
    "       'temperature_in', 'KBI_speed', 'soc', 'latitude',\n",
    "       'longitude', 'seatbelt_codriver', 'seatbelt_rear_l',\n",
    "       'seatbelt_rear_r', 'street_category', 'altitude',\n",
    "       'datetime', 'session', 'time_second',\n",
    "       'distance_driven', 'weekday'\n",
    "]\n",
    "\n",
    "bad_quality = ['CHA_ESP_drive_mode', \n",
    "             'CHA_MO_drive_mode',\n",
    "             'rain_sensor',\n",
    "             'kickdown',\n",
    "             'ESP_speed',\n",
    "             'seatbelt_rear_m',\n",
    "            'driving_program',\n",
    "            'ts_normalized'\n",
    "             ]\n",
    "\n",
    "dynamic_context_var = ['avg_irradiation', 'steering_speed', 'temperature_out', \n",
    "                       'light_sensor_rear', 'light_sensor_front', \n",
    "                       'temperature_in', 'KBI_speed', 'soc', 'latitude',\n",
    "                       'longitude',  'street_category', 'altitude','time_second',\n",
    "                       'distance_driven']\n",
    "static_context_var = ['car_id', 'month', 'weekday','hour', 'seatbelt_codriver', 'seatbelt_rear_l',\n",
    "                       'seatbelt_rear_r',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(vehicle):\n",
    "    df = pd.read_csv(os.path.join(PATH_TO_LOAD, vehicle + \"_merged.csv\"), parse_dates=['datetime'], low_memory=False)\n",
    "    context_lists = dynamic_context_var + static_context_var + ['session', 'datetime']\n",
    "    context_lists.remove('car_id')\n",
    "    df_filt = df[context_lists]\n",
    "    df_filt = df_filt.dropna(subset=['KBI_speed'])\n",
    "    df_filt_sort = df_filt.sort_values(by=['session','datetime'])\n",
    "    return df_filt_sort\n",
    "\n",
    "vehicles = ['SEB880','SEB882','SEB883','SEB885','SEB888','SEB889']\n",
    "context_data = pd.DataFrame()\n",
    "\n",
    "if merge_context_data == True:\n",
    "    for vehicle in tqdm(vehicles):\n",
    "        context_curr = load_context(vehicle)\n",
    "        context_curr['car_id'] = vehicle\n",
    "        context_data = pd.concat([context_data, context_curr], axis=0)\n",
    "    context_data.to_csv(combined_context_path)\n",
    "\n",
    "if regenerate_context_data == True:\n",
    "    context_data = pd.read_csv(combined_context_path, parse_dates=['datetime'], index_col=0)\n",
    "    vehicle_list = context_data.car_id.unique().tolist()\n",
    "    vehicle_dict = {vehicle: random.randint(1, 50) for vehicle in vehicle_list}\n",
    "    context_data['car_id'] = context_data['car_id'].map(vehicle_dict)\n",
    "    context_data = context_data.sort_values(by=['session','datetime'])\n",
    "    context_data['session'] = context_data['session'].astype(int)\n",
    "    # static_context_var.append('session')\n",
    "    # static_context = context_data[static_context_var].drop_duplicates(subset=['car_id', 'session'])\n",
    "    dynamic_context_var.extend(['session', 'datetime'])\n",
    "    context_data = context_data[dynamic_context_var + static_context_var]\n",
    "    context_var = [item for item in dynamic_context_var + static_context_var if item not in ['session', 'datetime']]\n",
    "    context_data = context_data.groupby(['session', 'datetime'])[context_var].mean().reset_index()\n",
    "\n",
    "    if model_test_run:\n",
    "        with open(train_session_path, 'rb') as pickle_file:\n",
    "            train_sessions = pickle.load(pickle_file)\n",
    "\n",
    "        with open(test_session_path, 'rb') as pickle_file:\n",
    "            test_sessions = pickle.load(pickle_file)\n",
    "            \n",
    "        additional_columns = ['session_id']\n",
    "        context_data = context_data[context_data['session'].isin(train_sessions + test_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 248.58it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_sequence = pd.read_csv(sequence_context_path, parse_dates=['datetime'], index_col=0).reset_index()\n",
    "selected_sequence['session'] = selected_sequence['session'].astype(int)\n",
    "\n",
    "min_datetime_indices = selected_sequence.groupby('session')['datetime'].idxmin()\n",
    "selected_sequence = selected_sequence.drop(min_datetime_indices)\n",
    "selected_sequence.reset_index(drop=True, inplace=True)\n",
    "context_data = context_data[context_data['session'].isin(selected_sequence.session.unique().tolist())]\n",
    "\n",
    "training_sequence_context = pd.DataFrame()\n",
    "selected_dfs = []\n",
    "for session in tqdm(selected_sequence['session'].unique().tolist()):\n",
    "    selected_sequence_curr = selected_sequence[selected_sequence['session']==session]\n",
    "    context_data_curr = context_data[context_data['session']==session]\n",
    "    context_data_curr = context_data_curr[context_data_curr['datetime']<=selected_sequence_curr['datetime'].max()]\n",
    "    selected_dfs.append(context_data_curr)\n",
    "    # training_sequence_context = pd.concat([training_sequence_context,context_data_curr], axis=0)\n",
    "\n",
    "training_sequence_context = pd.concat(selected_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1:\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "Tensor 2:\n",
      "tensor([8, 8])\n",
      "\n",
      "Concatenated Tensor along dimension 0:\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor1 = torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
    "tensor2 = torch.tensor([8, 8])\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "concatenated_tensor = torch.cat((tensor1, tensor2), dim=0)\n",
    "\n",
    "print(\"Tensor 1:\")\n",
    "print(tensor1)\n",
    "\n",
    "print(\"\\nTensor 2:\")\n",
    "print(tensor2)\n",
    "\n",
    "print(\"\\nConcatenated Tensor along dimension 0:\")\n",
    "print(concatenated_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([4, 8, 2, 7, 1, 9, 3])\n",
      "\n",
      "Top 3 Values:\n",
      "tensor([9, 8, 7])\n",
      "\n",
      "Indices of Top 3 Values:\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "tensor = torch.tensor([[-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05],\n",
    "        [-6.8802e-03, -4.1273e-03,  1.3144e-02, -3.5430e-03, -2.4565e-03,\n",
    "         -3.8868e-03, -3.3703e-03, -7.3480e-03, -5.5187e-05]])\n",
    "\n",
    "# Specify the number of top elements to retrieve\n",
    "k = 3\n",
    "\n",
    "# Use torch.topk to get the indices of the top k values\n",
    "top_values, top_indices = torch.topk(tensor, k)\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "\n",
    "print(\"\\nTop {} Values:\".format(k))\n",
    "print(top_values)\n",
    "import numpy as np\n",
    "print(\"\\nIndices of Top {} Values:\".format(k))\n",
    "print(np.mean(top_indices.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 114.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# testing_sessions = [16, 25]\n",
    "# selected_sequence = selected_sequence[selected_sequence['session'].isin(testing_sessions)]\n",
    "# training_sequence_context = training_sequence_context[training_sequence_context['session'].isin(testing_sessions)]\n",
    "\n",
    "training_sequence_context_augmented = pd.DataFrame()\n",
    "session_id = 0\n",
    "if sequence_augmentation == True:\n",
    "    # for session in tqdm(selected_sequence['session'].unique().tolist()):\n",
    "    #     selected_sequence_curr = selected_sequence[selected_sequence['session']==session].reset_index()\n",
    "    #     context_curr = training_sequence_context[training_sequence_context['session']==session].reset_index()\n",
    "    #     for i in range(len(selected_sequence_curr)-1, -1, -1):\n",
    "    #         context_filt_curr = training_sequence_context[\n",
    "    #             (training_sequence_context['datetime'] <= selected_sequence_curr.loc[i, 'datetime'])].copy()\n",
    "    #         if whole_session_context == False:\n",
    "    #             context_filt_curr = context_filt_curr.tail(window)\n",
    "    #         # context_filt_curr.loc[context_filt_curr.index, 'session_id'] = session_id\n",
    "    #         context_filt_curr['session_id'] = session_id\n",
    "    #         training_sequence_context_augmented = pd.concat([training_sequence_context_augmented, context_filt_curr], axis=0)\n",
    "    #         session_id += 1\n",
    "    # context_data = training_sequence_context_augmented\n",
    "    grouped_selected_sequence = selected_sequence.groupby('session')\n",
    "    augmented_frames = []\n",
    "\n",
    "    for session, selected_sequence_curr in tqdm(grouped_selected_sequence):\n",
    "        for i, row in selected_sequence_curr.iloc[::-1].iterrows():\n",
    "            context_filt_curr = training_sequence_context[\n",
    "                (training_sequence_context['session'] == session) &\n",
    "                (training_sequence_context['datetime'] <= row['datetime'])\n",
    "            ].copy()\n",
    "            if not whole_session_context:\n",
    "                context_filt_curr = context_filt_curr.tail(window)\n",
    "            context_filt_curr['session_id'] = session_id\n",
    "\n",
    "            augmented_frames.append(context_filt_curr)\n",
    "            session_id += 1\n",
    "    training_sequence_context_augmented = pd.concat(augmented_frames, axis=0)\n",
    "    context_data = training_sequence_context_augmented\n",
    "\n",
    "else:\n",
    "    # if sequence_augmentation is set to false\n",
    "    if whole_session_context == True:\n",
    "        context_data = training_sequence_context\n",
    "    else:\n",
    "        context_data = training_sequence_context.groupby('session').tail(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/1ntcg5gj7nvf5wh_s2w95yh80000gn/T/ipykernel_22263/3409487816.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_dynamic_context['session_id'] = train_dynamic_context.groupby('session_id').ngroup()\n",
      "/var/folders/n8/1ntcg5gj7nvf5wh_s2w95yh80000gn/T/ipykernel_22263/3409487816.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dynamic_context['session_id'] = test_dynamic_context.groupby('session_id').ngroup()\n",
      "/var/folders/n8/1ntcg5gj7nvf5wh_s2w95yh80000gn/T/ipykernel_22263/3409487816.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_static_context['session'] = range(len(train_static_context))\n",
      "/var/folders/n8/1ntcg5gj7nvf5wh_s2w95yh80000gn/T/ipykernel_22263/3409487816.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_static_context['session'] = range(len(test_static_context))\n"
     ]
    }
   ],
   "source": [
    "# in the dynamic context there are some windows with time instances are less than 100 handle somehow\n",
    "with open(train_session_path, 'rb') as pickle_file:\n",
    "    train_sessions = pickle.load(pickle_file)\n",
    "\n",
    "with open(test_session_path, 'rb') as pickle_file:\n",
    "    test_sessions = pickle.load(pickle_file)\n",
    "    \n",
    "additional_columns = ['session_id']\n",
    "context_data = context_data[context_data['session'].isin(train_sessions + test_sessions)]\n",
    "# static_context = static_context[static_context['session'].isin(train_sessions + test_sessions)]\n",
    "static_context = context_data[static_context_var + additional_columns + ['session']]\n",
    "static_context = static_context.drop_duplicates()\n",
    "dynamic_context = context_data[dynamic_context_var + additional_columns]\n",
    "\n",
    "train_dynamic_context = dynamic_context[dynamic_context['session'].isin(train_sessions)]\n",
    "test_dynamic_context = dynamic_context[dynamic_context['session'].isin(test_sessions)]\n",
    "train_static_context = static_context[static_context['session'].isin(train_sessions)]\n",
    "test_static_context = static_context[static_context['session'].isin(test_sessions)]\n",
    "\n",
    "if sequence_augmentation:\n",
    "    train_dynamic_context['session_id'] = train_dynamic_context.groupby('session_id').ngroup()\n",
    "    test_dynamic_context['session_id'] = test_dynamic_context.groupby('session_id').ngroup()\n",
    "else:\n",
    "    train_dynamic_context['session_id'] = train_dynamic_context.groupby('session').ngroup()\n",
    "    test_dynamic_context['session_id'] = test_dynamic_context.groupby('session').ngroup()\n",
    "\n",
    "train_dynamic_context = train_dynamic_context.drop(columns=['session'])\n",
    "test_dynamic_context = test_dynamic_context.drop(columns=['session'])\n",
    "\n",
    "dynamic_context_to_normalize = [col for col in train_dynamic_context.columns if col not in ['session_ids', 'datetime', 'session_id', 'session']]\n",
    "scaler_dynamic_context = MinMaxScaler()\n",
    "scaler_dynamic_context.fit(train_dynamic_context[dynamic_context_to_normalize])\n",
    "train_dynamic_context[dynamic_context_to_normalize] = scaler_dynamic_context.transform(train_dynamic_context[dynamic_context_to_normalize])\n",
    "test_dynamic_context[dynamic_context_to_normalize] = scaler_dynamic_context.transform(test_dynamic_context[dynamic_context_to_normalize])\n",
    "\n",
    "train_static_context['session'] = range(len(train_static_context))\n",
    "test_static_context['session'] = range(len(test_static_context))\n",
    "\n",
    "# static_context_to_normalize = [col for col in train_static_context.columns if col not in ['session_ids', 'datetime', 'session_id', 'session']]\n",
    "# scaler_static_context = MinMaxScaler()\n",
    "# scaler_static_context.fit(train_static_context[static_context_to_normalize])\n",
    "# train_static_context[static_context_to_normalize] = scaler_static_context.transform(train_static_context[static_context_to_normalize])\n",
    "# test_static_context[static_context_to_normalize] = scaler_static_context.transform(test_static_context[static_context_to_normalize])\n",
    "\n",
    "train_static_context = train_static_context.drop(columns=['session_id'])\n",
    "test_static_context = test_static_context.drop(columns=['session_id'])\n",
    "\n",
    "train_dynamic_context.to_csv(train_dynamic_context_path, index=False)\n",
    "test_dynamic_context.to_csv(test_dynamic_context_path, index=False)\n",
    "\n",
    "train_static_context.to_csv(train_static_context_path, index=False)\n",
    "test_static_context.to_csv(test_static_context_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 2\n",
      "18 2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_static_context.session.unique().tolist()), len(test_static_context.session.unique().tolist()))\n",
    "print(len(train_dynamic_context.session_id.unique().tolist()), len(test_dynamic_context.session_id.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_id</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>seatbelt_codriver</th>\n",
       "      <th>seatbelt_rear_l</th>\n",
       "      <th>seatbelt_rear_r</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>969165</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968779</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968770</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968558</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967632</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045587</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042378</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039701</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248797</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248735</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236322</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224820</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224210</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221294</th>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478465</th>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797651</th>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7121195</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7121098</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         car_id  month  weekday  seatbelt_codriver  seatbelt_rear_l  \\\n",
       "969165     11.0    6.0      5.0                0.0              0.0   \n",
       "968779     11.0    6.0      5.0                0.0              0.0   \n",
       "968770     11.0    6.0      5.0                0.0              0.0   \n",
       "968558     11.0    6.0      5.0                0.0              0.0   \n",
       "967632     11.0    6.0      5.0                0.0              0.0   \n",
       "1045587    11.0    6.0      3.0                0.0              0.0   \n",
       "1042378    11.0    6.0      3.0                0.0              0.0   \n",
       "1039701    11.0    6.0      3.0                0.0              0.0   \n",
       "2248797    26.0   10.0      1.0                0.0              0.0   \n",
       "2248735    26.0   10.0      1.0                0.0              0.0   \n",
       "2236322    26.0   10.0      1.0                0.0              0.0   \n",
       "2224820    26.0   10.0      1.0                0.0              0.0   \n",
       "2224210    26.0   10.0      1.0                0.0              0.0   \n",
       "2221294    26.0   10.0      1.0                0.0              0.0   \n",
       "2478465    26.0   11.0      6.0                1.0              0.0   \n",
       "2797651    26.0   12.0      5.0                1.0              0.0   \n",
       "7121195     2.0    2.0      6.0                0.0              0.0   \n",
       "7121098     2.0    2.0      6.0                0.0              0.0   \n",
       "\n",
       "         seatbelt_rear_r  session  \n",
       "969165               0.0        0  \n",
       "968779               0.0        1  \n",
       "968770               0.0        2  \n",
       "968558               0.0        3  \n",
       "967632               0.0        4  \n",
       "1045587              0.0        5  \n",
       "1042378              0.0        6  \n",
       "1039701              0.0        7  \n",
       "2248797              0.0        8  \n",
       "2248735              0.0        9  \n",
       "2236322              0.0       10  \n",
       "2224820              0.0       11  \n",
       "2224210              0.0       12  \n",
       "2221294              0.0       13  \n",
       "2478465              0.0       14  \n",
       "2797651              0.0       15  \n",
       "7121195              0.0       16  \n",
       "7121098              0.0       17  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_static_context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carsii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
