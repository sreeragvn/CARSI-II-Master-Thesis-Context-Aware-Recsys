{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.98it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 20.18it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "cross_validation_path = '../datasets/sequential/cross_validation'\n",
    "path = '../datasets/sequential/featengg'\n",
    "# parameter_path = '../datasets/sequential/cross_validation'\n",
    "input_type = ['dense_static_context', 'dynamic_context', 'static_context']\n",
    "dense_static_context_var =  ['distance_driven_benchmark', 'soc', 'time_second'] \n",
    "data_type =  ['dense_static_context', 'dynamic_context', 'static_context', 'seq']\n",
    "\n",
    "input_data_train = []\n",
    "input_data_test = []\n",
    "column_names = []\n",
    "\n",
    "for i, input_typ in enumerate(input_type):\n",
    "    data_train = pd.read_csv(path+'/'+input_type[i]+'/unnormal/train.csv')\n",
    "    data_test = pd.read_csv(path+'/'+input_type[i]+'/unnormal/test.csv')\n",
    "    column_names.append(data_test.columns.tolist())\n",
    "    input_data_train.append(data_train)\n",
    "    input_data_test.append(data_test)\n",
    "\n",
    "data_train = pd.read_csv(path+'/seq/train.tsv', sep='\\t')\n",
    "data_test = pd.read_csv(path+'/seq/test.tsv', sep='\\t')\n",
    "column_names.append(data_test.columns.tolist())\n",
    "input_data_train.append(data_train)\n",
    "input_data_test.append(data_test)\n",
    "\n",
    "# for data in input_data_test:\n",
    "#     print(data.columns)\n",
    "\n",
    "window_car_id_train = input_data_train[2][['window_id', 'car_id']].drop_duplicates()\n",
    "window_car_id_test = input_data_test[2][['window_id', 'car_id']].drop_duplicates()\n",
    "max_window_id = input_data_train[0].window_id.max()\n",
    "\n",
    "dataset_train, dataset_test = [], []\n",
    "for i, (train, test) in enumerate(zip(input_data_train, input_data_test)):\n",
    "    if i == 2:\n",
    "        dataset_train.append(train)\n",
    "        dataset_test.append(test)\n",
    "        continue\n",
    "    dataset_train.append(pd.merge(train, window_car_id_train, on='window_id', how='inner'))\n",
    "    dataset_test.append(pd.merge(test, window_car_id_test, on='window_id', how='inner'))\n",
    "\n",
    "for i, test in enumerate(dataset_test):\n",
    "    test['window_id'] += int(max_window_id) + 1\n",
    "    dataset_test[i] = test\n",
    "\n",
    "dataset = []\n",
    "for i, (train, test) in enumerate(zip(dataset_train, dataset_test)):\n",
    "    data = pd.concat([train, test], axis=0)\n",
    "    dataset.append(data)\n",
    "\n",
    "drivers = dataset[0].car_id.unique().tolist()\n",
    "\n",
    "cross_val_data = {}\n",
    "for driver in tqdm(drivers):\n",
    "    data_driver = {}\n",
    "    for typ, data in zip(data_type, dataset):\n",
    "        data_test = data[data['car_id']==driver]\n",
    "        data_train = data[data['car_id']!=driver]\n",
    "        \n",
    "        if typ == 'dynamic_context':\n",
    "            data_test = data_test.sort_values(by=['session', 'window_id','datetime'])\n",
    "            data_train = data_train.sort_values(by=['session', 'window_id','datetime'])\n",
    "        else:\n",
    "            data_test = data_test.sort_values(by=['session','window_id'])\n",
    "            data_train = data_train.sort_values(by=['session','window_id'])\n",
    "\n",
    "        data_test['wind_id'] = data_test.groupby(['session', 'window_id']).ngroup()\n",
    "        data_train['wind_id'] = data_train.groupby(['session', 'window_id']).ngroup()\n",
    "        data_test = data_test.drop(columns='window_id')\n",
    "        data_train = data_train.drop(columns='window_id')\n",
    "        data_test = data_test.rename(columns={\"wind_id\": \"window_id\"})\n",
    "        data_train = data_train.rename(columns={\"wind_id\": \"window_id\"})\n",
    "\n",
    "        data_driver[typ] = {'train': data_train, 'test': data_test}\n",
    "    cross_val_data[int(driver)] = data_driver\n",
    "\n",
    "#Normalisation\n",
    "def normalize_data(data, columns, scaler):\n",
    "    data.loc[:, columns] = scaler.transform(data[columns])\n",
    "    return data\n",
    "\n",
    "for driver in tqdm(drivers):\n",
    "    for context in ['dynamic_context', 'dense_static_context']:\n",
    "        train_context = cross_val_data[driver][context]['train']\n",
    "        test_context = cross_val_data[driver][context]['test']\n",
    "\n",
    "        if context == 'dynamic_context':\n",
    "            columns_to_normalize = [col for col in train_context.columns if col not in ['window_id', 'wind_id', 'session_ids', 'datetime', 'session_id', 'session', 'car_id']]\n",
    "        else:\n",
    "            columns_to_normalize = dense_static_context_var\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        scaler.fit(train_context[columns_to_normalize])\n",
    "        cross_val_data[driver][context]['train'] =  normalize_data(train_context, columns_to_normalize, scaler)\n",
    "        cross_val_data[driver][context]['test'] = normalize_data(test_context, columns_to_normalize, scaler)\n",
    "\n",
    "def class_weight(data, driver_path):\n",
    "    class_frequencies = data['item_id_target'].value_counts(normalize=True)\n",
    "    total_samples = len(data)\n",
    "    class_weights = {label: total_samples / (len(class_frequencies) * freq) for label, freq in class_frequencies.items()}\n",
    "    class_weights[0] = 0\n",
    "    class_weights[23] = 0\n",
    "    sorted_class_weights = dict(sorted(class_weights.items()))\n",
    "    class_weights_tensor_list = torch.tensor(list(sorted_class_weights.values()))\n",
    "    dir = os.path.join(cross_validation_path, driver_path, 'parameters')\n",
    "    file_path = os.path.join(dir, 'param.pkl')\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(class_weights_tensor_list, f)\n",
    "\n",
    "for driver in tqdm(drivers):\n",
    "    for data in ['train', 'test']:\n",
    "        driver_path = \"driver_\"+str(int(driver))    \n",
    "        for columns, typ in zip(column_names, data_type):\n",
    "            # print(cross_validation_path, driver_path, typ, data)\n",
    "            dir_path = os.path.join(cross_validation_path, driver_path, typ)\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            df = cross_val_data[driver][typ][data][columns]\n",
    "            if typ == 'seq':\n",
    "                file_path = os.path.join(dir_path, f'{data}.tsv')\n",
    "                df.to_csv(file_path, sep='\\t', index=False)\n",
    "                if data == 'test':\n",
    "                    class_weight(df, driver_path)\n",
    "            else:\n",
    "                file_path = os.path.join(dir_path, f'{data}.csv')\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "# cross_val_data[6]['static_context']['train']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carsii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
