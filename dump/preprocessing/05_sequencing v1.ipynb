{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LOAD = '../data/04_Merged'\n",
    "sequence_split = False\n",
    "# if not sequence_split:\n",
    "#     random_split = True\n",
    "random_split = True\n",
    "session_dict_generation = False\n",
    "vehicle_ident = False\n",
    "merge_context_data = False\n",
    "regenerate_context_data = True\n",
    "\n",
    "drive_mode_mapping = {\n",
    "    '0.0': 'Normal',\n",
    "    '1.0': 'Sport',\n",
    "    '2.0': 'Super Sport',\n",
    "    '3.0': 'Range',\n",
    "    '4.0': 'Gravel / Offroad'\n",
    "}\n",
    "\n",
    "climate_label = {\n",
    "    'clima': 'climate',\n",
    "    'AC': 'air conditioning'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {\n",
    "    'label': [],\n",
    "    'id':[],\n",
    "    'function_value': []\n",
    "}\n",
    "for function_value in df['FunctionValue'].unique().tolist():\n",
    "    dict1['id'].append(df[df['FunctionValue']==function_value]['ID'].unique().tolist())\n",
    "    dict1['function_value'].append(function_value)\n",
    "    dict1['label'].append(df[df['FunctionValue']==function_value]['Label'].unique().tolist())\n",
    "\n",
    "data = pd.DataFrame(dict1)\n",
    "data.to_csv('dict1.csv', sep='|')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(vehicle):\n",
    "    df = pd.read_csv(os.path.join(PATH_TO_LOAD, vehicle + \"_merged.csv\"), parse_dates=['datetime'], low_memory=False)\n",
    "    df_filt = df.drop(['index', 'avg_irradiation', 'steering_speed', 'temperature_out', 'hour',\n",
    "                        'month', 'odometer', 'light_sensor_rear', 'light_sensor_front',\n",
    "                        'temperature_in', 'KBI_speed', 'soc', 'ESP_speed', 'latitude',\n",
    "                        'longitude', 'seatbelt_codriver', 'seatbelt_rear_l', 'seatbelt_rear_m',\n",
    "                        'seatbelt_rear_r', \n",
    "                        'rain_sensor', 'street_category', 'kickdown', 'altitude',\n",
    "                        'driving_program', 'CHA_MO_drive_mode',\n",
    "                        'distance_driven', 'ts_normalized', 'weekday', 'CHA_ESP_drive_mode'], axis=1, inplace=False)\n",
    "    df_filt = df_filt.dropna(subset=['Label'])\n",
    "    df_filt_sort = df_filt.sort_values(by=['session','datetime'])\n",
    "    return df_filt_sort\n",
    "\n",
    "def replace_label(label):\n",
    "    match = re.search(r'\\d+\\.\\d+', label)\n",
    "    if match:\n",
    "        numeric_part = match.group()\n",
    "        replacement = drive_mode_mapping.get(numeric_part, numeric_part)\n",
    "        return label.replace(numeric_part, replacement)\n",
    "    else:\n",
    "        return label\n",
    "    \n",
    "def custom_concat(row):\n",
    "    # Check if the \"Label\" column value is \"media/selectedSource/Bluetooth\"\n",
    "    if row['Label'] == 'media/selectedSource/Bluetooth':\n",
    "        return f\"{row['Label']}:{row['FunctionValue']}\"\n",
    "    else:\n",
    "        return row['Label']\n",
    "\n",
    "def remove_consecutive_duplicates(row):\n",
    "    words = row.split()  # Split the row into words\n",
    "    unique_words = [words[0]]  # Initialize a list with the first word\n",
    "\n",
    "    # Iterate through the words and add non-consecutive duplicates to the list\n",
    "    for word in words[1:]:\n",
    "        if word != unique_words[-1]:\n",
    "            unique_words.append(word)\n",
    "\n",
    "    return ' '.join(unique_words) \n",
    "\n",
    "def itemization(df):\n",
    "    # Apply the function to the \"Label\" column\n",
    "    \n",
    "    df['Label'] = df['Label'].apply(replace_label)\n",
    "    df['Label'] = df['Label'].replace(climate_label)\n",
    "    df['Label'] = df['Label'].replace({'car/ESS/on' : '/car/eSoundSystem/on'})\n",
    "    df['Label'] = df['Label'].replace({'clima/AC/off' : 'climate/AirConditioning/off'})\n",
    "    df['Label'] = df['Label'].replace({'clima/AC/on' : 'climate/AirConditioning/on'})\n",
    "    df['Label'] = df['Label'].replace({'clima/AC/ECO' : 'climate/AirConditioning/ECO'})\n",
    "    df['Label'] = df.apply(custom_concat, axis=1)\n",
    "    condition = df['FunctionValue'] == 'Bluetooth::BLUETOOTH'\n",
    "    df['Label'] = df['Label'].str.replace('ðŸ¦–', '')\n",
    "    df['Label'] = df['Label'].str.replace('car/driveMode/0', 'car/driveMode/Normal')\n",
    "    df['Label'] = df['Label'].str.replace('car/driveMode/2', 'car/driveMode/Super Sport')\n",
    "    df['Label'] = df['Label'].str.replace('car/driveMode/3', 'car/driveMode/Range')\n",
    "    df['Label'] = df['Label'].apply(remove_consecutive_duplicates)\n",
    "    df['Label'] = df['Label'].str.replace('media/selectedSource/Favorite', \n",
    "                                                            'media/selectedSource/Favorite:TUNER_FAVORITES')\n",
    "    df['Label'] = df['Label'].str.replace('media/selectedSource/CarPlay', \n",
    "                                                            'media/selectedSource/Apple CarPlay')\n",
    "    df['Label'] = df['Label'].str.replace('phone/Start/CarPlay', \n",
    "                                                            'phone/Start/Apple CarPlay')\n",
    "    df.loc[condition, 'Label'] = \"media/selectedSource/Bluetooth\"\n",
    "    df['Label'] = df['Label'].str.replace('media/selectedSource/Bluetooth:Bluetooth::BLUETOOTH',\n",
    "                                                            'media/selectedSource/Bluetooth')\n",
    "\n",
    "\n",
    "    # Display the updated DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if session_dict_generation == True:\n",
    "    actions = []\n",
    "    combined_df = pd.DataFrame()\n",
    "    vehicles = ['SEB880','SEB882','SEB883','SEB885','SEB888','SEB889']\n",
    "    for vehicle in tqdm(vehicles):\n",
    "        df_filt_sort = load_df(vehicle)\n",
    "        df_filt_sort['vehicle'] = vehicle\n",
    "        processed_df = itemization(df_filt_sort)\n",
    "        actions.append(processed_df['Label'].unique().tolist())\n",
    "        combined_df = pd.concat([combined_df, processed_df], ignore_index=True)#\n",
    "\n",
    "    actions_unique = sorted(list(set([item for sublist in actions for item in sublist])))\n",
    "    actions_unique_dict = {element: random.randint(1, 120) for element in actions_unique}\n",
    "    combined_df.to_csv(\"../data/05_Interaction_Sequences/01_vehicles_merged.csv\")\n",
    "\n",
    "    with open('../data/05_Interaction_Sequences/infotainment_interaction_dict.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(actions_unique_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/05_Interaction_Sequences/infotainment_interaction_dict.pkl', 'rb') as pickle_file:\n",
    "    actions_unique_dict = pickle.load(pickle_file)\n",
    "\n",
    "combined_df = pd.read_csv(\"../data/05_Interaction_Sequences/01_vehicles_merged.csv\", parse_dates=['datetime'], index_col=0)\n",
    "combined_df = combined_df[['session','Label','vehicle', 'datetime']].sort_values(by = ['session', 'datetime'])\n",
    "# Calculate the time difference within each session and convert it to seconds\n",
    "vehicle_list = combined_df.vehicle.unique().tolist()\n",
    "vehicle_dict = {vehicle: random.randint(1, 50) for vehicle in vehicle_list}\n",
    "combined_df['vehicle'] = combined_df['vehicle'].map(vehicle_dict)\n",
    "\n",
    "with open('../data/05_Interaction_Sequences/vehicle_dict.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(vehicle_dict, pickle_file)\n",
    "\n",
    "combined_df['interaction_time_delta'] = (combined_df.groupby('session')['datetime'].diff().dt.total_seconds()/60).round(3)\n",
    "\n",
    "# If you want to handle the first row in each session separately, you can fill NaN values with 0\n",
    "combined_df['interaction_time_delta'] = combined_df['interaction_time_delta'].fillna(0)\n",
    "\n",
    "combined_df['Label_tokens'] = combined_df['Label'].map(actions_unique_dict)\n",
    "\n",
    "serialized_time_delta = combined_df.groupby('session')['interaction_time_delta'].agg(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "serialized_time_delta['interaction_time_delta_list'] = serialized_time_delta['interaction_time_delta'].str.split()\n",
    "\n",
    "if sequence_split:\n",
    "    serialized_time_delta['interaction_time_delta_list'] = serialized_time_delta['interaction_time_delta_list'].apply(lambda x: x[:-2])\n",
    "else:\n",
    "    serialized_time_delta['interaction_time_delta_list'] = serialized_time_delta['interaction_time_delta_list'].apply(lambda x: x[:-1])\n",
    "serialized_time_delta['interaction_time_delta_train'] = serialized_time_delta['interaction_time_delta_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "serialized_df = combined_df.groupby('session')['Label_tokens'].agg(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "session_vehicle_dict = dict(zip(combined_df['session'], combined_df['vehicle']))\n",
    "\n",
    "serialized_df['Label_tokens_list'] = serialized_df['Label_tokens'].str.split()\n",
    "serialized_df['Label_tokens_list_int'] = serialized_df['Label_tokens_list'].apply(lambda x: list(map(int, x)))\n",
    "serialized_df = serialized_df[serialized_df['Label_tokens_list_int'].apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "serialized_df['item_id_seq_test'] = serialized_df['Label_tokens_list'].apply(lambda x: ' '.join(x[:-1]))\n",
    "serialized_df['item_id_test'] = serialized_df['Label_tokens_list'].apply(lambda x: x[-1] if len(x) > 0 else None)\n",
    "serialized_df = serialized_df.drop(columns=['Label_tokens_list'])\n",
    "serialized_df = serialized_df[serialized_df['item_id_seq_test'] != ''].drop(columns=['Label_tokens'])\n",
    "\n",
    "serialized_df['Label_tokens_list'] = serialized_df['item_id_seq_test'].str.split()\n",
    "serialized_df['item_id_seq_train'] = serialized_df['Label_tokens_list'].apply(lambda x: ' '.join(x[:-1]))\n",
    "serialized_df['item_id_train'] = serialized_df['Label_tokens_list'].apply(lambda x: x[-1] if len(x) > 0 else None)\n",
    "serialized_df = serialized_df.drop(columns=['Label_tokens_list'])\n",
    "if sequence_split:\n",
    "    serialized_df = serialized_df[serialized_df['item_id_seq_train'] != '']\n",
    "else:\n",
    "    serialized_df = serialized_df[serialized_df['item_id_seq_test'] != '']\n",
    "\n",
    "serialized_df['vehicle'] = serialized_df['session'].map(session_vehicle_dict)\n",
    "serialized_df = serialized_df.drop(columns=['vehicle'])\n",
    "#serialized_df.rename(columns={'session': 'session_id'}, inplace=True)\n",
    "# serialized_df['session_id'] = range(len(serialized_df))\n",
    "# serialized_df['session_id'] = serialized_df['session_id'].astype(int)\n",
    "\n",
    "merged_df = serialized_df.merge(serialized_time_delta, on='session', how='inner')\n",
    "merged_df = merged_df.merge(combined_df[['session', 'vehicle']].drop_duplicates(), on='session', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sequence_split == True:\n",
    "    test_df = merged_df[['session_id', 'item_id_seq_test', 'item_id_test', 'interaction_time_delta_train']]\n",
    "    train_df = merged_df[['session_id', 'item_id_seq_train', 'item_id_train', 'interaction_time_delta_train']]\n",
    "    test_df.to_csv('../datasets/sequential/carsii_timedelta_seq/test.tsv', sep='\\t', index=False)\n",
    "    train_df.to_csv('../datasets/sequential/carsii_timedelta_seq/train.tsv', sep='\\t', index=False)\n",
    "elif random_split == True:\n",
    "    # merged_df = merged_df[['session', 'session_id', 'item_id_seq_test', 'item_id_test', 'interaction_time_delta_train']]\n",
    "    merged_df = merged_df[['session', 'item_id_seq_test', 'item_id_test', 'interaction_time_delta_train']]\n",
    "    train_df, test_df = train_test_split(merged_df, test_size=0.2)\n",
    "\n",
    "    train_sessions = train_df['session'].unique().tolist()\n",
    "    test_sessions = test_df['session'].unique().tolist()\n",
    "\n",
    "    train_df = train_df.sort_values(by='session')\n",
    "    test_df = test_df.sort_values(by='session')\n",
    "\n",
    "    train_df = train_df.drop(['session'], axis=1)\n",
    "    test_df = test_df.drop(['session'], axis=1)\n",
    "    \n",
    "    test_df['session_id'] = range(len(test_df))\n",
    "    test_df['session_id'] = test_df['session_id'].astype(int)\n",
    "\n",
    "    train_df['session_id'] = range(len(train_df))\n",
    "    train_df['session_id'] = train_df['session_id'].astype(int)\n",
    "    \n",
    "    train_df = train_df[['session_id'] + [col for col in train_df.columns if col != 'session_id']]\n",
    "    test_df = test_df[['session_id'] + [col for col in test_df.columns if col != 'session_id']]\n",
    "\n",
    "    # test_df['session'] = test_df['session'].astype(int)\n",
    "    # train_df['session'] = train_df['session'].astype(int)\n",
    "\n",
    "    test_df.to_csv('../datasets/sequential/carsii_timedelta_rand_seq/test.tsv', sep='\\t', index=False)\n",
    "    train_df.to_csv('../datasets/sequential/carsii_timedelta_rand_seq/train.tsv', sep='\\t', index=False)\n",
    "elif vehicle_ident == True:\n",
    "    merged_df = merged_df[['session_id', 'item_id_seq_test', 'item_id_test', 'interaction_time_delta_train', 'vehicle']]\n",
    "    train_df, test_df = train_test_split(merged_df, test_size=0.2)\n",
    "    test_df['session_id'] = range(len(test_df))\n",
    "    test_df['session_id'] = test_df['session_id'].astype(int)\n",
    "\n",
    "    train_df['session_id'] = range(len(train_df))\n",
    "    train_df['session_id'] = train_df['session_id'].astype(int)\n",
    "\n",
    "    test_df.to_csv('../datasets/sequential/carsii_timedelta_rand_seq_vehident/test.tsv', sep='\\t', index=False)\n",
    "    train_df.to_csv('../datasets/sequential/carsii_timedelta_rand_seq_vehident/train.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create context data\n",
    "\n",
    "def load_context(vehicle):\n",
    "    df = pd.read_csv(os.path.join(PATH_TO_LOAD, vehicle + \"_merged.csv\"), parse_dates=['datetime'], low_memory=False)\n",
    "    df_filt = df[['session','datetime', 'KBI_speed']]\n",
    "    df_filt = df_filt.dropna(subset=['KBI_speed'])\n",
    "    df_filt_sort = df_filt.sort_values(by=['session','datetime'])\n",
    "    return df_filt_sort\n",
    "\n",
    "vehicles = ['SEB880','SEB882','SEB883','SEB885','SEB888','SEB889']\n",
    "context_data = pd.DataFrame()\n",
    "\n",
    "if merge_context_data == True:\n",
    "    for vehicle in tqdm(vehicles):\n",
    "        context_curr = load_context(vehicle)\n",
    "        context_curr['car_id'] = vehicle\n",
    "        context_data = pd.concat([context_data, context_curr], axis=0)\n",
    "    context_data.to_csv('../data/05_Interaction_Sequences/context.csv')\n",
    "\n",
    "if regenerate_context_data == True:\n",
    "    selected_sessions = merged_df['session'].unique().tolist()\n",
    "    context_data = pd.read_csv('../data/05_Interaction_Sequences/context.csv', parse_dates=['datetime'], index_col=0)\n",
    "\n",
    "    vehicle_list = context_data.car_id.unique().tolist()\n",
    "    vehicle_dict = {vehicle: random.randint(1, 50) for vehicle in vehicle_list}\n",
    "    context_data['car_id'] = context_data['car_id'].map(vehicle_dict)\n",
    "\n",
    "    train_context_data = context_data[context_data['session'].isin(train_sessions)]\n",
    "    test_context_data = context_data[context_data['session'].isin(test_sessions)]\n",
    "\n",
    "    # test_context_data['session_id'] = range(len(test_context_data))\n",
    "    # test_context_data['session_id'] = test_context_data['session_id'].astype(int)\n",
    "\n",
    "    # train_context_data['session_id'] = range(len(train_context_data))\n",
    "    # train_context_data['session_id'] = train_context_data['session_id'].astype(int)\n",
    "\n",
    "    train_context_data = train_context_data.sort_values(by=['session','datetime'])\n",
    "    test_context_data = test_context_data.sort_values(by=['session','datetime'])\n",
    "\n",
    "    test_context_data['session'] = test_context_data['session'].astype(int)\n",
    "    # Create a new column 'session_id' based on consecutive integers within each session\n",
    "    test_context_data['session_id'] = test_context_data.groupby('session').ngroup()\n",
    "\n",
    "    train_context_data['session'] = train_context_data['session'].astype(int)\n",
    "    train_context_data['session_id'] = train_context_data.groupby('session').ngroup()\n",
    "\n",
    "    # train_context_data['session'] = train_context_data['session'].astype(int)\n",
    "    # test_context_data['session'] = test_context_data['session'].astype(int)\n",
    "\n",
    "    train_context_data = train_context_data.drop(['session'], axis=1)\n",
    "    test_context_data = test_context_data.drop(['session'], axis=1)\n",
    "\n",
    "    train_context_data = train_context_data[['session_id'] + [col for col in train_context_data.columns if col != 'session_id']]\n",
    "    test_context_data = test_context_data[['session_id'] + [col for col in test_context_data.columns if col != 'session_id']]\n",
    "\n",
    "    selected_context = ['KBI_speed', 'car_id']\n",
    "    train_context_data = train_context_data.groupby(['session_id', 'datetime'])[selected_context].mean().reset_index()\n",
    "    test_context_data = test_context_data.groupby(['session_id', 'datetime'])[selected_context].mean().reset_index()\n",
    "    \n",
    "    train_context_data.to_csv('../datasets/sequential/carsii_timedelta_rand_seq/context/train.csv', index=False)\n",
    "    test_context_data.to_csv('../datasets/sequential/carsii_timedelta_rand_seq/context/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pd.read_csv('../datasets/sequential/carsii_timedelta_rand_seq/context/test.csv', parse_dates=['datetime'])\n",
    "selected_context = ['KBI_speed', 'car_id']\n",
    "# context['hour_minute'] = context['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "context['hour_minute'] = context['datetime'].dt.strftime('%Y-%m-%d %H')\n",
    "context = context.drop(['datetime'], axis=1)\n",
    "context = context.groupby(['session_id', 'hour_minute'])[selected_context].mean().reset_index()\n",
    "context = context.drop(['hour_minute'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carsii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
